{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5D19HIbDatE",
        "outputId": "3ab6a59c-a08c-4244-c4c6-c1b34d829cc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (3.9.1)\n",
            "Requirement already satisfied: click in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from click->nltk) (0.4.6)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: spacy in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (0.15.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.2.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (75.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: colorama in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
            "Requirement already satisfied: wrapt in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\users\\gaura\\onedrive\\documents\\nlp\\.conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'nltk' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall nltk\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall spacy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[43mnltk\u001b[49m\u001b[38;5;241m.\u001b[39mdownload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpunkt_tab\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nltk' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "%pip install nltk\n",
        "%pip install spacy\n",
        "nltk.download('punkt_tab')\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "Lemma = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "pGJeH2crDz5u",
        "outputId": "89d52d24-2960-4df8-d490-e75880292e1f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Port St. Lucie, Florida</td>\n",
              "      <td>Port St. Lucie is a city in St. Lucie County, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Dirty Dozen</td>\n",
              "      <td>Dirty Dozen may refer to:\\n\\nBooks, film and t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Glenrothes</td>\n",
              "      <td>Glenrothes (; , ; ; ) is a town situated in th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Roughnecks: Starship Troopers Chronicles</td>\n",
              "      <td>Roughnecks: Starship Troopers Chronicles is a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>June 13</td>\n",
              "      <td>\\n\\nEvents\\n\\nPre-1600\\n 313 – The decisions o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13874</th>\n",
              "      <td>Northport, New York</td>\n",
              "      <td>Northport is a historic maritime village on th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13875</th>\n",
              "      <td>Ko</td>\n",
              "      <td>A KO is a knockout in various sports, such as ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>Rudy Ruettiger</td>\n",
              "      <td>Daniel Eugene \"Rudy\" Ruettiger (born August 23...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13877</th>\n",
              "      <td>Chicago Cubs</td>\n",
              "      <td>The Chicago Cubs are an American professional ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13878</th>\n",
              "      <td>Wilf Kirkham</td>\n",
              "      <td>Wilfred Thomas Kirkham (26 November 1901 – 20 ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13879 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          title  \\\n",
              "0                       Port St. Lucie, Florida   \n",
              "1                                   Dirty Dozen   \n",
              "2                                    Glenrothes   \n",
              "3      Roughnecks: Starship Troopers Chronicles   \n",
              "4                                       June 13   \n",
              "...                                         ...   \n",
              "13874                       Northport, New York   \n",
              "13875                                        Ko   \n",
              "13876                            Rudy Ruettiger   \n",
              "13877                              Chicago Cubs   \n",
              "13878                              Wilf Kirkham   \n",
              "\n",
              "                                                    text  \n",
              "0      Port St. Lucie is a city in St. Lucie County, ...  \n",
              "1      Dirty Dozen may refer to:\\n\\nBooks, film and t...  \n",
              "2      Glenrothes (; , ; ; ) is a town situated in th...  \n",
              "3      Roughnecks: Starship Troopers Chronicles is a ...  \n",
              "4      \\n\\nEvents\\n\\nPre-1600\\n 313 – The decisions o...  \n",
              "...                                                  ...  \n",
              "13874  Northport is a historic maritime village on th...  \n",
              "13875  A KO is a knockout in various sports, such as ...  \n",
              "13876  Daniel Eugene \"Rudy\" Ruettiger (born August 23...  \n",
              "13877  The Chicago Cubs are an American professional ...  \n",
              "13878  Wilfred Thomas Kirkham (26 November 1901 – 20 ...  \n",
              "\n",
              "[13879 rows x 2 columns]"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "df = pd.read_csv(r\"train(1).csv\")\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "tyPGes6DKeua"
      },
      "outputs": [],
      "source": [
        "df = df.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "v8NKDr39R82f"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\",\"tok2vec\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "13879it [24:54,  9.29it/s]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "pre_corpus=[]\n",
        "# # Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\",\"tok2vec\"])\n",
        "\n",
        "texts = list(df)\n",
        "\n",
        "# Process the texts\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=50)):\n",
        "    # Perform lemmatization and stopword removal\n",
        "    lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    \n",
        "    # Join the tokens back into a string\n",
        "    processed_text = \" \".join(lemmatized_tokens)\n",
        "    \n",
        "    # Print the processed text\n",
        "    pre_corpus.append(processed_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(pre_corpus, columns=[\"Text\"])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"pre_corpus.csv\", index=False)  # Set index=False to avoid unnecessary index column\n",
        "\n",
        "print(\"CSV file saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100it [00:10,  9.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSV file saved successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "test_set = pd.read_csv(\"test(1).csv\")\n",
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "df_test = pd.read_csv(\"test(1).csv\")\n",
        "df_test=df_test.text\n",
        "pre_corpus=[]\n",
        "\n",
        "texts = list(df_test)\n",
        "\n",
        "# Process the texts\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=50)):\n",
        "    # Perform lemmatization and stopword removal\n",
        "    lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "    \n",
        "    # Join the tokens back into a string\n",
        "    processed_text = \" \".join(lemmatized_tokens)\n",
        "    \n",
        "    # Print the processed text\n",
        "    pre_corpus.append(processed_text)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(pre_corpus, columns=[\"Text\"])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"test_pre_corpus.csv\", index=False)  # Set index=False to avoid unnecessary index column\n",
        "\n",
        "print(\"CSV file saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {},
      "outputs": [],
      "source": [
        "df2 = pd.read_csv(\"pre_corpus.csv\")  # Ensure df2 is the correct dataset\n",
        "corpus = df2[\"Text\"].dropna().tolist() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>port lucie city lucie county florida united st...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>dirty dozen refer books film television dirty ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>glenrothes town situated heart fife east centr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>roughnecks starship troopers chronicles cgi an...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>events decisions edict milan signed constantin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13874</th>\n",
              "      <td>northport historic maritime village northern s...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13875</th>\n",
              "      <td>ko knockout sports boxing martial arts ko kō r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13876</th>\n",
              "      <td>daniel eugene rudy ruettiger born august motiv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13877</th>\n",
              "      <td>chicago cubs american professional baseball te...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13878</th>\n",
              "      <td>wilfred thomas kirkham november october englis...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>13879 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    Text\n",
              "0      port lucie city lucie county florida united st...\n",
              "1      dirty dozen refer books film television dirty ...\n",
              "2      glenrothes town situated heart fife east centr...\n",
              "3      roughnecks starship troopers chronicles cgi an...\n",
              "4      events decisions edict milan signed constantin...\n",
              "...                                                  ...\n",
              "13874  northport historic maritime village northern s...\n",
              "13875  ko knockout sports boxing martial arts ko kō r...\n",
              "13876  daniel eugene rudy ruettiger born august motiv...\n",
              "13877  chicago cubs american professional baseball te...\n",
              "13878  wilfred thomas kirkham november october englis...\n",
              "\n",
              "[13879 rows x 1 columns]"
            ]
          },
          "execution_count": 146,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## N-gram Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13779/13779 [00:17<00:00, 774.43it/s]\n",
            "100%|██████████| 13779/13779 [00:32<00:00, 428.35it/s]\n",
            "100%|██████████| 13779/13779 [00:40<00:00, 336.56it/s]\n",
            "100%|██████████| 13779/13779 [00:19<00:00, 724.96it/s]\n",
            "100%|██████████| 12985/12985 [00:00<00:00, 1091356.15it/s]\n",
            "100%|██████████| 13779/13779 [00:32<00:00, 427.63it/s]\n",
            "100%|██████████| 5565837/5565837 [00:03<00:00, 1453471.17it/s]\n",
            "100%|██████████| 13779/13779 [00:39<00:00, 349.06it/s]\n",
            "100%|██████████| 12427057/12427057 [00:08<00:00, 1482983.48it/s]\n",
            "100%|██████████| 13779/13779 [00:20<00:00, 666.49it/s]\n",
            "100%|██████████| 12985/12985 [00:00<00:00, 828713.29it/s]\n",
            "100%|██████████| 13779/13779 [00:34<00:00, 397.55it/s]\n",
            "100%|██████████| 5565837/5565837 [00:06<00:00, 797367.37it/s]\n",
            "100%|██████████| 13779/13779 [00:43<00:00, 318.15it/s]\n",
            "100%|██████████| 12427057/12427057 [00:20<00:00, 605748.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Vocabulary Size: 12985\n",
            "Sample Unigram Probabilities: [(('port',), 0.00035390552787373), (('city',), 0.005551092171740339), (('county',), 0.002033806125824797), (('florida',), 0.00043003564494182233), (('united',), 0.003233601843343998)]\n",
            "Sample Bigram Probabilities: [(('port', 'city'), 0.01182168951646006), (('city', 'county'), 0.009549681329014731), (('county', 'florida'), 0.007023727428294128), (('united', 'states'), 0.487054960839974), (('county', 'population'), 0.008454013740964932)]\n",
            "Sample Trigram Probabilities: [(('combined', 'statistical', 'area'), 0.09879839786381843), (('households', 'children', 'age'), 0.6559139784946236), (('children', 'age', 'living'), 0.6492867683226758), (('age', 'living', 'married'), 0.6236559139784946), (('living', 'married', 'couples'), 0.6504653567735263)]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from collections import Counter, defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "def preprocess_text(text):\n",
        "    return text.split()\n",
        "\n",
        "def calculate_token_frequencies(corpus):\n",
        "    # Flatten the list of sentences into a single list of tokens\n",
        "    all_tokens = [token for sentence in corpus for token in sentence]\n",
        "    return Counter(all_tokens)\n",
        "\n",
        "def filter_vocabulary_by_threshold(corpus, threshold=0.01):\n",
        "    # Calculate the token frequencies across the corpus\n",
        "    token_frequencies = calculate_token_frequencies(corpus)\n",
        "    \n",
        "    # Calculate the total number of tokens in the corpus\n",
        "    total_tokens = sum(token_frequencies.values())\n",
        "    \n",
        "    # Apply threshold filtering\n",
        "    filtered_vocab = {token for token, freq in token_frequencies.items() if (freq ) >= (threshold*len(corpus))}\n",
        "    \n",
        "    return filtered_vocab, len(filtered_vocab)\n",
        "\n",
        "def create_ngram_counts(corpus, n, filtered_vocab):\n",
        "    ngram_counts = defaultdict(int)\n",
        "    context_counts = defaultdict(int)\n",
        "    \n",
        "    for tokens in tqdm(corpus):\n",
        "        # Only use tokens that are in the filtered vocabulary\n",
        "        tokens = [token for token in tokens if token in filtered_vocab]\n",
        "        \n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i + n])\n",
        "            context = tuple(tokens[i:i + n - 1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "def train_ngram_model(corpus, n, filtered_vocab,v):\n",
        "    ngram_counts, context_counts = create_ngram_counts(corpus, n, filtered_vocab)\n",
        "    \n",
        "    probabilities = {}\n",
        "    for ngram, count in tqdm(ngram_counts.items()):\n",
        "        context = ngram[:-1]\n",
        "        context_key = tuple(context) if context else ()\n",
        "        prob = (count + 1) / (context_counts.get(context_key,0) + len(v))\n",
        "        if count >= (0.01 * len(corpus)):  # 1% threshold\n",
        "            probabilities[ngram] = prob\n",
        "    \n",
        "    return probabilities\n",
        "\n",
        "def filter_ngram_dict(corpus, n, filtered_vocab):\n",
        "    ngram_counts, context_counts = create_ngram_counts(corpus, n, filtered_vocab)\n",
        "    \n",
        "    filter_ngram = {}\n",
        "    for ngram, count in tqdm(ngram_counts.items()):\n",
        "        context = ngram[:-1]\n",
        "        context_key = tuple(context) if context else ()\n",
        "        if count >= (0.01 * len(corpus)):  # 1% threshold\n",
        "            filter_ngram[ngram] = count\n",
        "\n",
        "    return filter_ngram\n",
        "\n",
        "\n",
        "corpus = df2[:-100].Text.dropna().tolist()\n",
        "processed_corpus = [preprocess_text(para) for para in corpus]\n",
        "\n",
        "# Filter vocabulary using a threshold (1% frequency threshold)\n",
        "filtered_vocab, filtered_vocab_size = filter_vocabulary_by_threshold(processed_corpus, threshold=0.01)\n",
        "\n",
        "#  n-gram counts using the filtered vocabulary\n",
        "unigram_counts, unigram_context = create_ngram_counts(processed_corpus, 1, filtered_vocab)\n",
        "bigram_counts, bigram_context = create_ngram_counts(processed_corpus, 2, filtered_vocab)\n",
        "trigram_counts, trigram_context = create_ngram_counts(processed_corpus, 3, filtered_vocab)\n",
        "\n",
        "# Get diffirent vocabulary Dictionaries\n",
        "unigram_v=filter_ngram_dict(processed_corpus,1,filtered_vocab)\n",
        "bigram_v = filter_ngram_dict(processed_corpus,2,filtered_vocab)\n",
        "trigram_v = filter_ngram_dict(processed_corpus,3,filtered_vocab)\n",
        "\n",
        "\n",
        "# Step 3: Train n-gram model with filtered vocabulary\n",
        "unigram_probs = train_ngram_model(processed_corpus, 1, filtered_vocab,unigram_v)\n",
        "bigram_probs = train_ngram_model(processed_corpus, 2, filtered_vocab,bigram_v)\n",
        "trigram_probs = train_ngram_model(processed_corpus, 3, filtered_vocab,trigram_v)\n",
        "\n",
        "print(\"Filtered Vocabulary Size:\", filtered_vocab_size)\n",
        "print(\"Sample Unigram Probabilities:\", list(unigram_probs.items())[:5])\n",
        "print(\"Sample Bigram Probabilities:\", list(bigram_probs.items())[:5])\n",
        "print(\"Sample Trigram Probabilities:\", list(trigram_probs.items())[:5])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perplexity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {},
      "outputs": [],
      "source": [
        "v_uni = unigram_v\n",
        "v_bi = bigram_v\n",
        "v_tri = trigram_v\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESJ58GpPwHQY",
        "outputId": "345b756b-2c58-4be4-d403-34437da943f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unigram Model Perplexity: 17105.445\n",
            "Bigram Model Perplexity: 4525.801\n",
            "Trigram Model Perplexity: 1110.731\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "\n",
        "\n",
        "def calculate_perplexity(test_corpus, ngram_probs, context_counts, n, vocab_size, k=1):\n",
        "    sum_perp = 0\n",
        "\n",
        "    for text in test_corpus:\n",
        "       \n",
        "        tokens = text.split()\n",
        "        text_log_prob = 0\n",
        "        text_words = len(tokens)\n",
        "        ##  assign different vocabularies\n",
        "        if(n==1):\n",
        "            vocab_size= len(v_uni)\n",
        "        elif(n==2):\n",
        "            vocab_size= len(v_bi)\n",
        "        else:\n",
        "            vocab_size= len(v_tri)\n",
        "        \n",
        "        for i in range(len(tokens) - n + 1):\n",
        "            ngram = tuple(tokens[i:i + n])\n",
        "            context = ngram[:-1]\n",
        "\n",
        "            if ngram in ngram_probs:\n",
        "                prob = ngram_probs[ngram]\n",
        "            else:                       ## if that n-gram was not seen\n",
        "                # Apply Laplace smoothing\n",
        "                numerator = k\n",
        "                denominator = context_counts.get(context, 0) + k * vocab_size\n",
        "                prob = numerator / denominator\n",
        "\n",
        "            text_log_prob += np.log(prob)\n",
        "\n",
        "        if text_words > 0:\n",
        "            avg_log_prob = text_log_prob / text_words\n",
        "            perplexity = np.exp(-avg_log_prob)\n",
        "            sum_perp += perplexity\n",
        "\n",
        "    return sum_perp / len(test_corpus)\n",
        "\n",
        "def compute_perplexities(df_test, unigram_probs, bigram_probs, trigram_probs, \n",
        "                         unigram_context, bigram_context, trigram_context, vocab_size):\n",
        "    test_corpus = df_test.Text.dropna().tolist()\n",
        "\n",
        "    unigram_perplexity = calculate_perplexity(test_corpus, unigram_probs, unigram_context, 1, v_uni)\n",
        "    bigram_perplexity = calculate_perplexity(test_corpus, bigram_probs, bigram_context, 2, v_bi)\n",
        "    trigram_perplexity = calculate_perplexity(test_corpus, trigram_probs, trigram_context, 3, v_tri)\n",
        "\n",
        "    print(f\"Unigram Model Perplexity: {unigram_perplexity:.3f}\")\n",
        "    print(f\"Bigram Model Perplexity: {bigram_perplexity:.3f}\")\n",
        "    print(f\"Trigram Model Perplexity: {trigram_perplexity:.3f}\")\n",
        "\n",
        "    return unigram_perplexity, bigram_perplexity, trigram_perplexity\n",
        "\n",
        "# Example usage\n",
        "perplexities = compute_perplexities(df2[-100:], unigram_probs, bigram_probs, trigram_probs, \n",
        "                                    unigram_context, bigram_context, trigram_context, vocab_size=v_uni)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {},
      "outputs": [],
      "source": [
        "validation_set=df2[-100:]\n",
        "test_set = pd.read_csv(\"test_pre_corpus.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>weyburn eleventh largest city saskatchewan can...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>catholic high school chs government aided auto...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>minnesota golden gophers commonly shortened go...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>following notable people born raised lived sig...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>theobald germanic dithematic composed elements...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>kutztown university pennsylvania kutztown univ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>paul william bear bryant september january ame...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>jonathan michael paul spector born march ameri...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>98</th>\n",
              "      <td>abia state state south east geopolitical zone ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>table rock village pawnee county nebraska unit...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>100 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 Text\n",
              "0   weyburn eleventh largest city saskatchewan can...\n",
              "1   catholic high school chs government aided auto...\n",
              "2   minnesota golden gophers commonly shortened go...\n",
              "3   following notable people born raised lived sig...\n",
              "4   theobald germanic dithematic composed elements...\n",
              "..                                                ...\n",
              "95  kutztown university pennsylvania kutztown univ...\n",
              "96  paul william bear bryant september january ame...\n",
              "97  jonathan michael paul spector born march ameri...\n",
              "98  abia state state south east geopolitical zone ...\n",
              "99  table rock village pawnee county nebraska unit...\n",
              "\n",
              "[100 rows x 1 columns]"
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Interpolation Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:04<00:00,  2.27it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.32it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.31it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.33it/s]\n",
            "100%|██████████| 10/10 [00:04<00:00,  2.30it/s]\n",
            "100%|██████████| 10/10 [00:43<00:00,  4.34s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal Weights: (np.float64(0.8888888888888888), np.float64(0.11111111111111116), np.float64(0.0)), Minimum Perplexity: 1018.31724069965\n",
            "Final Test Perplexity: 997.0694520648213\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_interpolated_prob(w1, w2, w3, weights):\n",
        "    lambda1, lambda2, lambda3 = weights\n",
        "    \n",
        "    trigram_prob = trigram_probs.get((w1, w2, w3), 1 / len(v_tri))\n",
        "    bigram_prob = bigram_probs.get((w2, w3), 1 / len(v_bi))\n",
        "    unigram_prob = unigram_probs.get((w3,), 1 / len(v_uni))\n",
        "    \n",
        "    return lambda1 * trigram_prob + lambda2 * bigram_prob + lambda3 * unigram_prob\n",
        "\n",
        "def comp_perplexity(corpus, weights):\n",
        "    lambda1, lambda2, lambda3 = weights\n",
        "    log_sum = 0.0\n",
        "    ngram_count = 0\n",
        "    \n",
        "    for para in corpus:\n",
        "        tokens = para.split()\n",
        "        for i in range(len(tokens) - 2):\n",
        "            w1, w2, w3 = tokens[i], tokens[i+1], tokens[i+2]\n",
        "            prob = compute_interpolated_prob(w1, w2, w3, (lambda1, lambda2, lambda3))\n",
        "            \n",
        "            if prob <= 0:\n",
        "                prob = 1e-15 \n",
        "            \n",
        "            log_sum += np.log(prob)\n",
        "            ngram_count += 1\n",
        "    \n",
        "    return np.exp(-log_sum / ngram_count) if ngram_count > 0 else float('inf')\n",
        "\n",
        "def optimal_weights(validation_texts):\n",
        "    optimal_perplexity = float('inf')\n",
        "    optimal_weights = (0, 0, 0)\n",
        "    \n",
        "    for lambda1 in tqdm(np.linspace(0, 1, 10)):\n",
        "        for lambda2 in tqdm(np.linspace(0, 1 - lambda1, 10)):\n",
        "            lambda3 = 1 - lambda1 - lambda2\n",
        "            perplexity = comp_perplexity(validation_texts, (lambda1, lambda2, lambda3))\n",
        "            \n",
        "            if perplexity < optimal_perplexity:\n",
        "                optimal_perplexity = perplexity\n",
        "                optimal_weights = (lambda1, lambda2, lambda3)\n",
        "    \n",
        "    return optimal_weights, optimal_perplexity\n",
        "\n",
        "optimal_lambdas, min_perplexity = optimal_weights(validation_set.Text)\n",
        "print(f\"Optimal Weights: {optimal_lambdas}, Minimum Perplexity: {min_perplexity}\")\n",
        "\n",
        "test_perplexity = comp_perplexity(test_set['Text'], optimal_lambdas)\n",
        "print(f\"Final Test Perplexity: {test_perplexity}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
