{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 11255501,
          "sourceType": "datasetVersion",
          "datasetId": 7034116
        },
        {
          "sourceId": 11314066,
          "sourceType": "datasetVersion",
          "datasetId": 7076718
        }
      ],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gau-ravv/ML/blob/main/notebook8e8c4d9055.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:05:54.043687Z",
          "iopub.execute_input": "2025-04-09T16:05:54.043938Z",
          "iopub.status.idle": "2025-04-09T16:05:54.052243Z",
          "shell.execute_reply.started": "2025-04-09T16:05:54.043917Z",
          "shell.execute_reply": "2025-04-09T16:05:54.051538Z"
        },
        "id": "iN7qONRYjfE9"
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "OK14DNw7jfFA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%pip install nltk\n",
        "%pip install spacy\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "stemmer = PorterStemmer()\n",
        "Lemma = WordNetLemmatizer()\n",
        "nltk.download('wordnet')\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "import pandas as pd"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:21.747145Z",
          "iopub.execute_input": "2025-04-09T16:06:21.747510Z",
          "iopub.status.idle": "2025-04-09T16:06:28.420140Z",
          "shell.execute_reply.started": "2025-04-09T16:06:21.747479Z",
          "shell.execute_reply": "2025-04-09T16:06:28.418917Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fDLnIHSxjfFA",
        "outputId": "32944010-f934-4414-a79a-4c0c52a9a12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import spacy\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\",\"tok2vec\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:35.336972Z",
          "iopub.execute_input": "2025-04-09T16:06:35.337357Z",
          "iopub.status.idle": "2025-04-09T16:06:36.014370Z",
          "shell.execute_reply.started": "2025-04-09T16:06:35.337305Z",
          "shell.execute_reply": "2025-04-09T16:06:36.013634Z"
        },
        "id": "pya9G3qvjfFC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv(\"/kaggle/input/nlp-assign/pre_corpus.csv\")  # Ensure df2 is the correct dataset\n",
        "corpus = df2[\"Text\"].dropna().tolist()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:38.815525Z",
          "iopub.execute_input": "2025-04-09T16:06:38.815817Z",
          "iopub.status.idle": "2025-04-09T16:06:40.642283Z",
          "shell.execute_reply.started": "2025-04-09T16:06:38.815793Z",
          "shell.execute_reply": "2025-04-09T16:06:40.641558Z"
        },
        "id": "QT9C5lJkjfFC"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df2"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:41.740829Z",
          "iopub.execute_input": "2025-04-09T16:06:41.741175Z",
          "iopub.status.idle": "2025-04-09T16:06:41.756723Z",
          "shell.execute_reply.started": "2025-04-09T16:06:41.741146Z",
          "shell.execute_reply": "2025-04-09T16:06:41.755757Z"
        },
        "id": "5SIqhUmXjfFD",
        "outputId": "7fe54bf3-ef26-451c-a145-5994b05f368c"
      },
      "outputs": [
        {
          "execution_count": 17,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                                    Text\n0      port lucie city lucie county florida united st...\n1      dirty dozen refer books film television dirty ...\n2      glenrothes town situated heart fife east centr...\n3      roughnecks starship troopers chronicles cgi an...\n4      events decisions edict milan signed constantin...\n...                                                  ...\n13874  northport historic maritime village northern s...\n13875  ko knockout sports boxing martial arts ko k≈ç r...\n13876  daniel eugene rudy ruettiger born august motiv...\n13877  chicago cubs american professional baseball te...\n13878  wilfred thomas kirkham november october englis...\n\n[13879 rows x 1 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>port lucie city lucie county florida united st...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dirty dozen refer books film television dirty ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glenrothes town situated heart fife east centr...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>roughnecks starship troopers chronicles cgi an...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>events decisions edict milan signed constantin...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13874</th>\n      <td>northport historic maritime village northern s...</td>\n    </tr>\n    <tr>\n      <th>13875</th>\n      <td>ko knockout sports boxing martial arts ko k≈ç r...</td>\n    </tr>\n    <tr>\n      <th>13876</th>\n      <td>daniel eugene rudy ruettiger born august motiv...</td>\n    </tr>\n    <tr>\n      <th>13877</th>\n      <td>chicago cubs american professional baseball te...</td>\n    </tr>\n    <tr>\n      <th>13878</th>\n      <td>wilfred thomas kirkham november october englis...</td>\n    </tr>\n  </tbody>\n</table>\n<p>13879 rows √ó 1 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "4x8Qu5VEjfFD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "0o1YnH-xjfFE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(r\"/kaggle/input/nlp-assign/train(1).csv\")\n",
        "df_title = df.title"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:44.095476Z",
          "iopub.execute_input": "2025-04-09T16:06:44.095838Z",
          "iopub.status.idle": "2025-04-09T16:06:46.641163Z",
          "shell.execute_reply.started": "2025-04-09T16:06:44.095808Z",
          "shell.execute_reply": "2025-04-09T16:06:46.640468Z"
        },
        "id": "m2JQTijIjfFE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "Bu-HGMsjjfFF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch\n",
        "%pip install rouge_score"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:06:53.448817Z",
          "iopub.execute_input": "2025-04-09T16:06:53.449125Z",
          "iopub.status.idle": "2025-04-09T16:07:00.048481Z",
          "shell.execute_reply.started": "2025-04-09T16:06:53.449098Z",
          "shell.execute_reply": "2025-04-09T16:07:00.047449Z"
        },
        "id": "z6woICgajfFG",
        "outputId": "e924c176-58ee-4db3-e1c8-5a65ee28114a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.12.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\nNote: you may need to restart the kernel to use updated packages.\nRequirement already satisfied: rouge_score in /usr/local/lib/python3.10/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge_score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge_score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge_score) (2024.2.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "pre_corpus=[]\n",
        "# # Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\",\"tok2vec\"])\n",
        "\n",
        "texts = list(df_title)\n",
        "\n",
        "# Process the texts\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=50)):\n",
        "    # Perform lemmatization and stopword removal\n",
        "    lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = \" \".join(lemmatized_tokens)\n",
        "\n",
        "    # Print the processed text\n",
        "    pre_corpus.append(processed_text)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:00.050253Z",
          "iopub.execute_input": "2025-04-09T16:07:00.050617Z",
          "iopub.status.idle": "2025-04-09T16:07:02.892594Z",
          "shell.execute_reply.started": "2025-04-09T16:07:00.050590Z",
          "shell.execute_reply": "2025-04-09T16:07:02.891607Z"
        },
        "id": "ivFJxpHCjfFG",
        "outputId": "324b1ebb-b868-4e42-9032-8f864ee91476"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n13879it [00:02, 6925.38it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "# Convert list to DataFrame\n",
        "df = pd.DataFrame(pre_corpus, columns=[\"title\"])\n",
        "\n",
        "# Save as CSV\n",
        "df.to_csv(\"pre_corpus_heading.csv\", index=False)  # Set index=False to avoid unnecessary index column\n",
        "\n",
        "print(\"CSV file saved successfully!\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:02.893502Z",
          "iopub.execute_input": "2025-04-09T16:07:02.893768Z",
          "iopub.status.idle": "2025-04-09T16:07:02.912438Z",
          "shell.execute_reply.started": "2025-04-09T16:07:02.893732Z",
          "shell.execute_reply": "2025-04-09T16:07:02.911741Z"
        },
        "id": "b2W0MmKLjfFH",
        "outputId": "d6a8bba7-e463-46f0-87d0-106d0a5780eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "CSV file saved successfully!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "df_title = df\n",
        "df_title"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:02.913345Z",
          "iopub.execute_input": "2025-04-09T16:07:02.913640Z",
          "iopub.status.idle": "2025-04-09T16:07:02.943300Z",
          "shell.execute_reply.started": "2025-04-09T16:07:02.913610Z",
          "shell.execute_reply": "2025-04-09T16:07:02.942632Z"
        },
        "id": "YFuushhPjfFH",
        "outputId": "c8163289-af25-441c-add5-abe5432e4681"
      },
      "outputs": [
        {
          "execution_count": 23,
          "output_type": "execute_result",
          "data": {
            "text/plain": "                                         title\n0                           port lucie florida\n1                                  dirty dozen\n2                                   glenrothes\n3      roughnecks starship troopers chronicles\n4                                         june\n...                                        ...\n13874                       northport new york\n13875                                       ko\n13876                           rudy ruettiger\n13877                             chicago cubs\n13878                             wilf kirkham\n\n[13879 rows x 1 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>port lucie florida</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>dirty dozen</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>glenrothes</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>roughnecks starship troopers chronicles</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>june</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>13874</th>\n      <td>northport new york</td>\n    </tr>\n    <tr>\n      <th>13875</th>\n      <td>ko</td>\n    </tr>\n    <tr>\n      <th>13876</th>\n      <td>rudy ruettiger</td>\n    </tr>\n    <tr>\n      <th>13877</th>\n      <td>chicago cubs</td>\n    </tr>\n    <tr>\n      <th>13878</th>\n      <td>wilf kirkham</td>\n    </tr>\n  </tbody>\n</table>\n<p>13879 rows √ó 1 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "pre_corpus=[]\n",
        "# # Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\",\"tok2vec\"])\n",
        "\n",
        "texts = list(pd.read_csv(r\"/kaggle/input/nlp-assign/test(1).csv\").title)\n",
        "\n",
        "# Process the texts\n",
        "for doc in tqdm(nlp.pipe(texts, batch_size=50)):\n",
        "    # Perform lemmatization and stopword removal\n",
        "    lemmatized_tokens = [token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct and token.is_alpha]\n",
        "\n",
        "    # Join the tokens back into a string\n",
        "    processed_text = \" \".join(lemmatized_tokens)\n",
        "\n",
        "    # Print the processed text\n",
        "    pre_corpus.append(processed_text)\n",
        "\n",
        "df_test_title = pd.DataFrame(pre_corpus, columns=[\"title\"])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:02.944600Z",
          "iopub.execute_input": "2025-04-09T16:07:02.944805Z",
          "iopub.status.idle": "2025-04-09T16:07:03.619096Z",
          "shell.execute_reply.started": "2025-04-09T16:07:02.944787Z",
          "shell.execute_reply": "2025-04-09T16:07:03.618269Z"
        },
        "id": "emjs6KIXjfFH",
        "outputId": "66977425-1616-4e73-d5e2-509c8ca0ccd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n  warnings.warn(Warnings.W111)\n100it [00:00, 4894.23it/s]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Define constants\n",
        "MAX_NEW_TOKENS = 10\n",
        "HIDDEN_DIM = 300\n",
        "SOS_TOKEN = 0\n",
        "EOS_TOKEN = 1\n",
        "PAD_TOKEN = 2\n",
        "UNK_TOKEN = 3\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function to create vocabulary\n",
        "# def create_vocabulary(paragraphs, titles, min_frequency=0.01):\n",
        "#     tokenized_words = [word for paragraph in paragraphs for word in paragraph.split()]\n",
        "#     word_counts = Counter(tokenized_words)\n",
        "#     total_texts = len(paragraphs)\n",
        "\n",
        "#     vocabulary = {\"<SOS>\": SOS_TOKEN, \"<EOS>\": EOS_TOKEN, \"<PAD>\": PAD_TOKEN, \"<UNK>\": UNK_TOKEN}\n",
        "#     vocabulary.update({\n",
        "#         word: idx + 4 for idx, (word, count) in enumerate(word_counts.items()) if (count / total_texts) >= min_frequency\n",
        "#     })\n",
        "#     return vocabulary\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "def create_vocabulary(paragraphs, titles_train_raw,min_frequency=0.01):\n",
        "    word_article_count = Counter()  # Track occurrences of words across different articles\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        unique_words = set(paragraph.split())  # Unique words in each article\n",
        "        word_article_count.update(unique_words)  # Count each word once per article\n",
        "\n",
        "    total_articles = len(paragraphs)  # Total number of articles\n",
        "\n",
        "    # Filter words that meet the minimum frequency criteria\n",
        "    filtered_words = {word: count for word, count in word_article_count.items() if (count / total_articles) >= min_frequency}\n",
        "\n",
        "    # Create vocabulary with only filtered words\n",
        "    vocabulary = {\"<SOS>\": SOS_TOKEN, \"<EOS>\": EOS_TOKEN, \"<PAD>\": PAD_TOKEN, \"<UNK>\": UNK_TOKEN}\n",
        "    vocabulary.update({word: idx + 4 for idx, word in enumerate(filtered_words)})\n",
        "\n",
        "    # print(f\"Number of words appearing in only one article: {sum(1 for count in filtered_words.values() if count == 1)}\")\n",
        "    print(f\"Vocabulary size: {len(vocabulary)}\")  # Debugging info\n",
        "\n",
        "    return vocabulary\n",
        "\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_text(texts, vocabulary):\n",
        "    return [[vocabulary.get(word, UNK_TOKEN) for word in text.split()] or [PAD_TOKEN] for text in texts]\n",
        "\n",
        "# Collate function for DataLoader\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "    inputs_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in inputs], batch_first=True, padding_value=PAD_TOKEN)\n",
        "    targets_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in targets], batch_first=True, padding_value=PAD_TOKEN)\n",
        "    return inputs_padded, targets_padded\n",
        "\n",
        "# Dataset class\n",
        "class ParagraphTitleDataset(Dataset):\n",
        "    def __init__(self, input_sequences, target_sequences):\n",
        "        self.input_sequences = input_sequences\n",
        "        self.target_sequences = target_sequences\n",
        "    def __len__(self):\n",
        "        return len(self.input_sequences)\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_sequences[idx], self.target_sequences[idx]\n",
        "\n",
        "# Define Encoder\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size, padding_idx=PAD_TOKEN)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    def forward(self, input_seq):\n",
        "        input_seq = torch.clamp(input_seq, min=0, max=self.embedding.num_embeddings - 1)\n",
        "        embedded = self.dropout(self.embedding(input_seq))\n",
        "        output, hidden = self.gru(embedded)\n",
        "        hidden = hidden.view(2, hidden.size(1), -1).permute(1, 0, 2).contiguous().view(hidden.size(1), -1)\n",
        "        return output, hidden\n",
        "\n",
        "# Define Decoder with ReLU layer after embedding\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
        "        self.relu = nn.ReLU()  # Added ReLU layer\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "    def forward(self, input_token, hidden):\n",
        "        input_token = torch.clamp(input_token, min=0, max=self.embedding.num_embeddings - 1)\n",
        "        embedded = self.embedding(input_token).unsqueeze(1)\n",
        "        embedded = self.relu(embedded)  # Apply ReLU activation\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        output = self.out(output.squeeze(1))\n",
        "        return output, hidden\n",
        "\n",
        "# Define Seq2Seq Model\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2SeqRNN, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.hidden_projection = nn.Linear(encoder.gru.hidden_size * 2, decoder.gru.hidden_size)\n",
        "    def forward(self, input_seq, target_seq=None):\n",
        "        batch_size = input_seq.size(0)\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq)\n",
        "        decoder_hidden = self.hidden_projection(encoder_hidden).unsqueeze(0)\n",
        "        decoder_input = torch.full((batch_size, 1), SOS_TOKEN, device=input_seq.device, dtype=torch.long)\n",
        "        if self.training and target_seq is not None:\n",
        "            target_len = target_seq.size(1)\n",
        "            outputs = torch.zeros(batch_size, target_len, self.decoder.out.out_features, device=input_seq.device)\n",
        "            for t in range(target_len):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input[:, -1], decoder_hidden)\n",
        "                outputs[:, t, :] = decoder_output\n",
        "                if t < target_len - 1:\n",
        "                    decoder_input = torch.cat([decoder_input, target_seq[:, t].unsqueeze(1)], dim=1)\n",
        "            return outputs\n",
        "        else:\n",
        "            outputs = []\n",
        "            for _ in range(MAX_NEW_TOKENS):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input[:, -1], decoder_hidden)\n",
        "                outputs.append(decoder_output)\n",
        "                top_token = decoder_output.argmax(dim=-1)\n",
        "                decoder_input = torch.cat([decoder_input, top_token.unsqueeze(1)], dim=1)\n",
        "                if torch.all(top_token == EOS_TOKEN):\n",
        "                    break\n",
        "            return torch.stack(outputs, dim=1)\n",
        "\n",
        "# Training function\n",
        "def train(model, data_loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for batch_count, (input_seq, target_seq) in enumerate(data_loader, 1):\n",
        "        input_seq, target_seq = input_seq.to(DEVICE), target_seq.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_seq, target_seq)\n",
        "        outputs_flat = outputs.contiguous().view(-1, outputs.size(-1))\n",
        "        target_flat = target_seq.contiguous().view(-1)\n",
        "\n",
        "        # Fix vocabulary indexing issue\n",
        "        max_vocab_index = max(vocabulary.values())\n",
        "        vocab_size = max_vocab_index + 1\n",
        "        max_target = target_flat.max().item()\n",
        "        if max_target >= vocab_size:\n",
        "            print(f\"Batch {batch_count}: Target index {max_target} exceeds vocab size {vocab_size}\")\n",
        "            continue\n",
        "\n",
        "        loss = criterion(outputs_flat, target_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    avg_loss = epoch_loss / len(data_loader)\n",
        "    print(f\"Training Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n",
        "# Evaluation function\n",
        "def evaluate(model, data_loader, vocabulary):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in data_loader:\n",
        "            input_seq = input_seq.to(DEVICE)\n",
        "            target_seq = target_seq.to(DEVICE)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(input_seq)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "            # Get predicted tokens\n",
        "            predicted_tokens = outputs.argmax(dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "            # Convert tokens to text\n",
        "            generated_texts = decode_outputs(predicted_tokens, vocabulary)\n",
        "            reference_texts = decode_outputs(target_seq, vocabulary)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            for i in range(len(reference_texts)):\n",
        "                scores_list.append(scorer.score(reference_texts[i], generated_texts[i]))\n",
        "\n",
        "    return scores_list\n",
        "\n",
        "# Decoding function\n",
        "def decode_outputs(output_sequences, vocabulary):\n",
        "    index_to_word = {idx: word for word, idx in vocabulary.items()}\n",
        "    decoded_texts = []\n",
        "\n",
        "    for sequence in output_sequences:\n",
        "        words = []\n",
        "        for idx in sequence:\n",
        "            token = idx.item()\n",
        "            if token == EOS_TOKEN:\n",
        "                break\n",
        "            if token != SOS_TOKEN and token != PAD_TOKEN:\n",
        "                words.append(index_to_word.get(token, \"<UNK>\"))\n",
        "        decoded_texts.append(\" \".join(words))\n",
        "\n",
        "    return decoded_texts\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    paragraphs_train_raw = df2[:-500].Text.tolist()\n",
        "    titles_train_raw = df_title[:-500].title.tolist()\n",
        "\n",
        "    vocabulary = create_vocabulary(paragraphs_train_raw, titles_train_raw, min_frequency=0.01)\n",
        "    max_vocab_index = max(vocabulary.values())\n",
        "    vocab_size = max_vocab_index + 1\n",
        "    print(f\"Vocabulary size: {vocab_size}\")\n",
        "\n",
        "    tokenized_paragraphs_train = tokenize_text(paragraphs_train_raw, vocabulary)\n",
        "    tokenized_titles_train = tokenize_text(titles_train_raw, vocabulary)\n",
        "\n",
        "    train_dataset = ParagraphTitleDataset(tokenized_paragraphs_train, tokenized_titles_train)\n",
        "    train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "    encoder = EncoderRNN(vocab_size, HIDDEN_DIM).to(DEVICE)\n",
        "    decoder = DecoderRNN(HIDDEN_DIM, vocab_size).to(DEVICE)\n",
        "    seq2seq_model = Seq2SeqRNN(encoder, decoder).to(DEVICE)\n",
        "    optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "    for epoch in tqdm(range(3)):\n",
        "        train(seq2seq_model, train_data_loader, optimizer, criterion)\n",
        "\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:18.858831Z",
          "iopub.execute_input": "2025-04-09T16:07:18.859151Z",
          "iopub.status.idle": "2025-04-09T16:07:18.893694Z",
          "shell.execute_reply.started": "2025-04-09T16:07:18.859117Z",
          "shell.execute_reply": "2025-04-09T16:07:18.892828Z"
        },
        "id": "4AJ8OvwvjfFI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary"
      ],
      "metadata": {
        "trusted": true,
        "id": "avsKg-jpjfFJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "U9v92PnejfFJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "F7K6PydsjfFJ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Updated Evaluation Function with title comparison\n",
        "def evaluate(model, data_loader, vocabulary):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores_list = []\n",
        "    all_predictions = []\n",
        "    all_references = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in data_loader:\n",
        "            input_seq = input_seq.to(DEVICE)\n",
        "            target_seq = target_seq.to(DEVICE)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(input_seq)  # (batch_size, seq_len, vocab_size)\n",
        "            predicted_tokens = outputs.argmax(dim=-1)  # (batch_size, seq_len)\n",
        "\n",
        "            # Decode predicted and reference tokens\n",
        "            generated_texts = decode_outputs(predicted_tokens, vocabulary)\n",
        "            reference_texts = decode_outputs(target_seq, vocabulary)\n",
        "\n",
        "            # Calculate ROUGE scores and print comparisons\n",
        "            for i in range(len(reference_texts)):\n",
        "                score = scorer.score(reference_texts[i], generated_texts[i])\n",
        "                scores_list.append(score)\n",
        "                all_predictions.append(generated_texts[i])\n",
        "                all_references.append(reference_texts[i])\n",
        "\n",
        "                # Print actual vs. predicted\n",
        "                # print(f\"\\nSample {len(scores_list)}:\")\n",
        "                # print(f\"  üü© Actual Title    : {reference_texts[i]}\")\n",
        "                # print(f\"  üü¶ Predicted Title : {generated_texts[i]}\")\n",
        "                # print(f\"  üìä ROUGE Scores    : {score}\")\n",
        "\n",
        "    return scores_list, all_predictions, all_references\n",
        "\n",
        "\n",
        "# Decoding function\n",
        "def decode_outputs(output_sequences, vocabulary):\n",
        "    index_to_word = {idx: word for word, idx in vocabulary.items()}\n",
        "    decoded_texts = []\n",
        "\n",
        "    for sequence in output_sequences:\n",
        "        words = []\n",
        "        for idx in sequence:\n",
        "            token = idx.item()\n",
        "            if token == EOS_TOKEN:\n",
        "                break\n",
        "            if token != SOS_TOKEN and token != PAD_TOKEN:\n",
        "                words.append(index_to_word.get(token, \"<UNK>\"))\n",
        "        decoded_texts.append(\" \".join(words))\n",
        "\n",
        "    return decoded_texts\n",
        "\n",
        "# df_testing = pd.read_csv(\"/kaggle/input/nlp-assign/test_pre_corpus.csv\")\n",
        "# df_testing = pd.read_csv(\"/kaggle/input/nlp-assign/pre_corpus.csv\")\n",
        "\n",
        "# df_testing_title = pd.read_csv(\"/kaggle/input/nlp-assign/test(1).csv\").title\n",
        "test_paragraphs_raw = df2[-500:].Text.tolist()   # Replace with tokenized test paragraphs.\n",
        "test_titles_raw = df_title[-500:].title.tolist()\n",
        "# test_titles_raw = df_test_title.title.tolist()\n",
        "# Step 1: Tokenize Test Data\n",
        "# test_paragraphs_raw = [\"Your test paragraph 1\", \"Your test paragraph 2\", ...]  # Replace with actual test data\n",
        "# test_titles_raw = [\"Title 1\", \"Title 2\", ...]  # Replace with actual titles\n",
        "\n",
        "# Convert test data into tokenized format using the same vocabulary as training\n",
        "tokenized_paragraphs_test = tokenize_text(test_paragraphs_raw, vocabulary)\n",
        "tokenized_titles_test = tokenize_text(test_titles_raw, vocabulary)\n",
        "\n",
        "# Step 2: Create Test Dataset and DataLoader\n",
        "test_dataset = ParagraphTitleDataset(tokenized_paragraphs_test, tokenized_titles_test)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n",
        "rouge_scores, predicted_titles, actual_titles = evaluate(seq2seq_model, test_data_loader, vocabulary)\n",
        "\n",
        "\n",
        "# # Step 4: Print Results\n",
        "# for i, score in enumerate(rouge_scores):\n",
        "#     print(f\"Sample {i+1} ROUGE Scores:\", score)\n",
        "\n",
        "avg_rouge = {\n",
        "    \"rouge1\": sum([s['rouge1'].fmeasure for s in rouge_scores]) / len(rouge_scores),\n",
        "    \"rouge2\": sum([s['rouge2'].fmeasure for s in rouge_scores]) / len(rouge_scores),\n",
        "    \"rougeL\": sum([s['rougeL'].fmeasure for s in rouge_scores]) / len(rouge_scores),\n",
        "}\n",
        "\n",
        "# Step 5: Print Average ROUGE Scores\n",
        "print(\"Average ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {avg_rouge['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {avg_rouge['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {avg_rouge['rougeL']:.4f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "f3qw_ePkjfFK"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "SgwMWBi4jfFL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B2"
      ],
      "metadata": {
        "id": "pYE_cBmzjfFL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    inputs, targets = zip(*batch)\n",
        "\n",
        "    # Pad input sequences\n",
        "    inputs_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in inputs], batch_first=True, padding_value=PAD_TOKEN)\n",
        "\n",
        "    # Pad target sequences (ensure target sequences are padded the same way)\n",
        "    targets_padded = pad_sequence([torch.tensor(seq, dtype=torch.long) for seq in targets], batch_first=True, padding_value=PAD_TOKEN)\n",
        "\n",
        "    return inputs_padded, targets_padded\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:35.387572Z",
          "iopub.execute_input": "2025-04-09T16:07:35.387897Z",
          "iopub.status.idle": "2025-04-09T16:07:35.392602Z",
          "shell.execute_reply.started": "2025-04-09T16:07:35.387867Z",
          "shell.execute_reply": "2025-04-09T16:07:35.391631Z"
        },
        "id": "qN1M4sQ6jfFM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from rouge_score import rouge_scorer\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def load_glove_embeddings(embedding_path, vocabulary, embedding_dim=300):\n",
        "    \"\"\"Loads pre-trained GloVe embeddings and returns an embedding matrix.\"\"\"\n",
        "    embeddings_index = {}\n",
        "\n",
        "    with open(embedding_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
        "            embeddings_index[word] = vector\n",
        "\n",
        "    embedding_matrix = np.random.normal(scale=0.6, size=(len(vocabulary), embedding_dim))\n",
        "    for word, idx in vocabulary.items():\n",
        "       if idx < vocab_size:  # Ensure index is within bounds\n",
        "        if word in embeddings_index:\n",
        "            embedding_matrix[idx] = embeddings_index[word]\n",
        "\n",
        "    return torch.tensor(embedding_matrix, dtype=torch.float)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:37.779658Z",
          "iopub.execute_input": "2025-04-09T16:07:37.779948Z",
          "iopub.status.idle": "2025-04-09T16:07:37.786202Z",
          "shell.execute_reply.started": "2025-04-09T16:07:37.779927Z",
          "shell.execute_reply": "2025-04-09T16:07:37.785249Z"
        },
        "id": "ZsBY_ApLjfFM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class HierEncoderRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size):\n",
        "        super(HierEncoderRNN, self).__init__()\n",
        "        # Embedding layer with padding_idx to ignore padding tokens during the embedding lookup\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size, padding_idx=PAD_TOKEN)\n",
        "\n",
        "        # Word-level GRU to process words within each sentence\n",
        "        self.word_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Sentence-level GRU to process the representation of sentences\n",
        "        self.sent_gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_seq):\n",
        "        # Apply embedding to input sequence\n",
        "        embedded = self.dropout(self.embedding(input_seq))\n",
        "\n",
        "        # Process words within each sentence using GRU\n",
        "        word_output, word_hidden = self.word_gru(embedded)\n",
        "\n",
        "        # Averaging word embeddings per sentence\n",
        "        sentence_embeddings = word_output.mean(dim=1, keepdim=True)\n",
        "\n",
        "        # Process sentence embeddings using sentence-level GRU\n",
        "        sent_output, sent_hidden = self.sent_gru(sentence_embeddings)\n",
        "\n",
        "        return sent_output, sent_hidden\n",
        "\n",
        "\n",
        "\n",
        "class Decoder2RNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, inference=False):\n",
        "        super(Decoder2RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size, padding_idx=PAD_TOKEN)\n",
        "        self.gru1 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.gru2 = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.inference = inference\n",
        "\n",
        "    def forward(self, input_token, hidden, inference=False):\n",
        "        # Handling input size for training vs inference\n",
        "        if inference:\n",
        "            # Make sure input_token is 2D for embedding lookup\n",
        "            if input_token.dim() == 0:  # Scalar tensor\n",
        "                input_token = input_token.unsqueeze(0)\n",
        "\n",
        "            # Create proper embedding with batch dimension\n",
        "            embedded = self.embedding(input_token)\n",
        "\n",
        "            # Make sure embedded has 3 dimensions [batch_size, seq_len, hidden_size]\n",
        "            if embedded.dim() == 2:\n",
        "                embedded = embedded.unsqueeze(1)  # Add sequence length dimension\n",
        "        else:\n",
        "            embedded = self.embedding(input_token).unsqueeze(1)  # Shape: (batch_size, 1, hidden_size)\n",
        "\n",
        "        # GRU operations\n",
        "        output, hidden1 = self.gru1(embedded, hidden)\n",
        "        output, hidden2 = self.gru2(output, hidden1)\n",
        "        output = self.softmax(self.out(output.squeeze(1)))  # Remove the second dimension (1) after GRU\n",
        "        return output, hidden2\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:40.572184Z",
          "iopub.execute_input": "2025-04-09T16:07:40.572530Z",
          "iopub.status.idle": "2025-04-09T16:07:40.581243Z",
          "shell.execute_reply.started": "2025-04-09T16:07:40.572498Z",
          "shell.execute_reply": "2025-04-09T16:07:40.580203Z"
        },
        "id": "MHfXHcnNjfFM"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import heapq\n",
        "\n",
        "class Seq2SeqRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, embedding_path=None,\n",
        "                 use_hierarchical=False, use_decoder2=False, use_beam_search=False, beam_width=3):\n",
        "        \"\"\"\n",
        "        A flexible Seq2Seq model with optional:\n",
        "        - GloVe Embeddings\n",
        "        - Hierarchical Encoding\n",
        "        - Double GRU Decoder\n",
        "        - Beam Search\n",
        "        \"\"\"\n",
        "        super(Seq2SeqRNN, self).__init__()\n",
        "\n",
        "        # Encoder selection\n",
        "        self.encoder = HierEncoderRNN(vocab_size, hidden_size) if use_hierarchical else EncoderRNN(vocab_size, hidden_size)\n",
        "\n",
        "        # Decoder selection\n",
        "        self.decoder = Decoder2RNN(hidden_size, vocab_size) if use_decoder2 else DecoderRNN(hidden_size, vocab_size)\n",
        "\n",
        "        # Beam Search\n",
        "        self.use_beam_search = use_beam_search\n",
        "        self.beam_width = beam_width\n",
        "\n",
        "         # Set inference_mode to False by default (indicating training mode)\n",
        "        # self.inference_mode = False\n",
        "\n",
        "        # Load GloVe embeddings if provided\n",
        "        if embedding_path:\n",
        "            self.load_embeddings(embedding_path)\n",
        "\n",
        "    def load_embeddings(self, embedding_path):\n",
        "        \"\"\"Loads GloVe embeddings into the encoder's embedding layer.\"\"\"\n",
        "        embedding_matrix = load_glove_embeddings(embedding_path, vocabulary)\n",
        "        self.encoder.embedding.weight.data.copy_(embedding_matrix)\n",
        "        self.encoder.embedding.weight.requires_grad = False  # Freeze embeddings\n",
        "\n",
        "    def forward(self, input_seq, target_seq=None):\n",
        "        input_seq = input_seq.to(DEVICE)\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        if self.use_beam_search and target_seq is None:\n",
        "            return self.beam_search_decoder(input_seq, decoder_hidden)\n",
        "\n",
        "        # Teacher forcing during training\n",
        "        decoder_input = torch.full((input_seq.size(0), 1), SOS_TOKEN, device=DEVICE)\n",
        "        outputs = []\n",
        "\n",
        "        for i in range(MAX_NEW_TOKENS):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input[:, -1], decoder_hidden)\n",
        "            outputs.append(decoder_output)\n",
        "            top_token = decoder_output.argmax(dim=1)\n",
        "            decoder_input = torch.cat([decoder_input, top_token.unsqueeze(1)], dim=1)\n",
        "\n",
        "            # Teacher forcing: Use target word if provided\n",
        "            if target_seq is not None and i < target_seq.shape[1]:\n",
        "                decoder_input[:, -1] = target_seq[:, i]\n",
        "\n",
        "            if torch.all(top_token == EOS_TOKEN):\n",
        "                break\n",
        "\n",
        "        outputs = torch.stack(outputs).transpose(0, 1)  # Shape: (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        # Ensure that the target sequence and output match in shape\n",
        "        max_len = target_seq.size(1) if target_seq is not None else outputs.size(1)\n",
        "        outputs = outputs[:, :max_len, :]  # Truncate if necessary to match target sequence length\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    def beam_search_decoder(self, input_seq, decoder_hidden, inference=True):\n",
        "        \"\"\"Performs Beam Search decoding.\"\"\"\n",
        "        batch_size = input_seq.size(0)\n",
        "        all_results = []\n",
        "\n",
        "        # Process each item in the batch separately\n",
        "        for b in range(batch_size):\n",
        "            # Extract the hidden state for this batch item only\n",
        "            # Original shape: [layers, batch_size, hidden_dim]\n",
        "            # We want: [layers, 1, hidden_dim]\n",
        "            single_hidden = decoder_hidden.clone()\n",
        "            single_hidden = single_hidden[:, b:b+1, :]  # Keep dimension with size 1\n",
        "\n",
        "            beams = [(0.0, [SOS_TOKEN], single_hidden)]  # (score, sequence, hidden_state)\n",
        "\n",
        "            for _ in range(MAX_NEW_TOKENS):\n",
        "                new_beams = []\n",
        "                for score, seq, hidden in beams:\n",
        "                    if seq[-1] == EOS_TOKEN:\n",
        "                        new_beams.append((score, seq, hidden))\n",
        "                        continue\n",
        "\n",
        "                    # Decoder input for the next token in the sequence\n",
        "                    decoder_input = torch.tensor([seq[-1]], device=DEVICE)\n",
        "\n",
        "                    # Pass through decoder\n",
        "                    decoder_output, new_hidden = self.decoder(decoder_input, hidden, inference=inference)\n",
        "\n",
        "                    # Get the top-k probabilities and indices\n",
        "                    topk_probs, topk_indices = torch.topk(decoder_output, self.beam_width)\n",
        "\n",
        "                    # Generate new beams with new sequences\n",
        "                    for i in range(self.beam_width):\n",
        "                        new_seq = seq + [topk_indices[0][i].item()]\n",
        "                        new_score = score + topk_probs[0][i].item()\n",
        "                        new_beams.append((new_score, new_seq, new_hidden))\n",
        "\n",
        "                # Sort beams by score and keep the top-k\n",
        "                beams = sorted(new_beams, key=lambda x: x[0], reverse=True)[:self.beam_width]\n",
        "\n",
        "                # Stop if all top beams end with EOS_TOKEN\n",
        "                if all(seq[-1] == EOS_TOKEN for _, seq, _ in beams[:min(3, len(beams))]):\n",
        "                    break\n",
        "\n",
        "            # Get best sequence for this batch item\n",
        "            best_seq = beams[0][1]\n",
        "            all_results.append(best_seq)\n",
        "\n",
        "        # Pad sequences to the same length\n",
        "        max_length = max(len(seq) for seq in all_results)\n",
        "        padded_results = [seq + [PAD_TOKEN] * (max_length - len(seq)) for seq in all_results]\n",
        "\n",
        "        # Convert to tensor\n",
        "        return torch.tensor(padded_results, device=DEVICE)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:43.249039Z",
          "iopub.execute_input": "2025-04-09T16:07:43.249365Z",
          "iopub.status.idle": "2025-04-09T16:07:43.263177Z",
          "shell.execute_reply.started": "2025-04-09T16:07:43.249312Z",
          "shell.execute_reply": "2025-04-09T16:07:43.262230Z"
        },
        "id": "WiZouLuOjfFN"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# ====== 1. Load Data ======\n",
        "# Replace this with actual dataset\n",
        "paragraphs_train_raw = df2[:-500].Text.tolist()  # Load training data\n",
        "titles_train_raw = df_title[:-500].title.tolist()  # Load corresponding titles\n",
        "\n",
        "paragraphs_test_raw = df2[-500:].Text.tolist()  # Load test data\n",
        "titles_test_raw = df_title[-500:].title.tolist()  # Load corresponding test titles\n",
        "\n",
        "# ====== 2. Create Vocabulary ======\n",
        "vocabulary = create_vocabulary(paragraphs_train_raw,titles_train_raw, min_frequency=0.01)\n",
        "vocab_size = len(vocabulary)\n",
        "\n",
        "# Tokenizing Data\n",
        "tokenized_paragraphs_train = tokenize_text(paragraphs_train_raw, vocabulary)\n",
        "tokenized_titles_train = tokenize_text(titles_train_raw, vocabulary)\n",
        "\n",
        "tokenized_paragraphs_test = tokenize_text(paragraphs_test_raw, vocabulary)\n",
        "tokenized_titles_test = tokenize_text(titles_test_raw, vocabulary)\n",
        "\n",
        "# Create Datasets & DataLoaders\n",
        "train_dataset = ParagraphTitleDataset(tokenized_paragraphs_train, tokenized_titles_train)\n",
        "test_dataset = ParagraphTitleDataset(tokenized_paragraphs_test, tokenized_titles_test)\n",
        "\n",
        "# train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn, drop_last=True)\n",
        "\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# ====== 3. Initialize Model ======\n",
        "seq2seq_model = Seq2SeqRNN(\n",
        "    vocab_size=vocab_size,\n",
        "    hidden_size=HIDDEN_DIM,\n",
        "    embedding_path=\"/kaggle/input/glove-data/glove.6B.300d.txt\",  # Provide GloVe path if available\n",
        "    use_hierarchical=True,  # Use hierarchical encoder\n",
        "    use_decoder2=True,  # Use dual-GRU decoder\n",
        "    use_beam_search=True,  # Enable beam search\n",
        "    beam_width=5  # Set beam width\n",
        ").to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(seq2seq_model.parameters(), lr=0.001)\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)  # Ignore padding during loss calculation\n",
        "\n",
        "# ====== 4. Training Function ======\n",
        "def train(model, data_loader, optimizer, criterion, num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for input_seq, target_seq in data_loader:\n",
        "            input_seq, target_seq = input_seq.to(DEVICE), target_seq.to(DEVICE)\n",
        "\n",
        "            # Ensure the target sequence and output match in length\n",
        "            max_len = max(input_seq.size(1), target_seq.size(1))\n",
        "            target_seq = target_seq[:, :max_len]  # Trim target sequence if necessary\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Model forward pass\n",
        "            outputs = model(input_seq, target_seq)\n",
        "\n",
        "            # print(outputs.size())\n",
        "            # print(target_seq.size())\n",
        "            # Ensure the output and target are the same shape\n",
        "            outputs = outputs.reshape(-1, outputs.size(-1))  # Flatten the output for loss calculation\n",
        "            target_seq = target_seq.view(-1)\n",
        "\n",
        "\n",
        "            # Apply the CrossEntropy loss while ignoring padding\n",
        "            loss = criterion(outputs, target_seq)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "# ====== 5. Evaluation Function ======\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# def evaluate(model, data_loader, vocabulary):\n",
        "#     model.eval()\n",
        "#     scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "#     scores_list = []\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         for input_seq, target_seq in data_loader:\n",
        "#             input_seq = input_seq.to(DEVICE)\n",
        "#             target_seq = target_seq.to(DEVICE)\n",
        "\n",
        "#             # Get model predictions\n",
        "#             outputs = model(input_seq)  # This should now be a tensor from beam_search_decoder\n",
        "\n",
        "#             # If the outputs are a list (from beam search), stack them into a tensor\n",
        "#             if isinstance(outputs, list):\n",
        "#                 outputs = torch.stack(outputs, dim=1)  # Stack the list into a 2D tensor\n",
        "\n",
        "#             # Ensure outputs are 2D: (batch_size, seq_len)\n",
        "#             if outputs.dim() == 1:  # If the output is 1D (e.g., from beam search)\n",
        "#                 outputs = outputs.unsqueeze(0)  # Make it 2D with shape [1, seq_len]\n",
        "\n",
        "#             # Perform argmax and slicing for sequence length\n",
        "#             generated_texts = decode_outputs(outputs.argmax(dim=-1)[:, :target_seq.size(1)], vocabulary)\n",
        "#             # generated_texts = decode_outputs(outputs.argmax(dim=-1), vocabulary)\n",
        "#             # generated_texts = decode_outputs(outputs.argmax(dim=-1), vocabulary)\n",
        "#             reference_texts = decode_outputs(target_seq, vocabulary)\n",
        "\n",
        "#             # Calculate ROUGE scores\n",
        "#             for i in range(len(reference_texts)):\n",
        "#                 scores_list.append(scorer.score(reference_texts[i], generated_texts[i]))\n",
        "\n",
        "#     avg_rouge = {\n",
        "#         \"rouge1\": sum([s['rouge1'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "#         \"rouge2\": sum([s['rouge2'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "#         \"rougeL\": sum([s['rougeL'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "#     }\n",
        "\n",
        "#     return avg_rouge\n",
        "\n",
        "def evaluate(model, data_loader, vocabulary):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_seq, target_seq in data_loader:\n",
        "            input_seq = input_seq.to(DEVICE)\n",
        "            target_seq = target_seq.to(DEVICE)\n",
        "\n",
        "            # Get model predictions\n",
        "            outputs = model(input_seq)  # This calls beam_search_decoder if needed\n",
        "\n",
        "            # Check dimensionality of outputs and handle accordingly\n",
        "            if outputs.dim() == 1:\n",
        "                # If outputs is 1D, it's a single sequence (convert to 2D with batch_size=1)\n",
        "                outputs = outputs.unsqueeze(0)\n",
        "\n",
        "            # Ensure outputs are the right shape for decoding\n",
        "            if outputs.dim() == 2:\n",
        "                # If no vocabulary dimension (just batch x sequence_length), assume these are indices\n",
        "                # No need for argmax - these are already the best token indices\n",
        "                generated_indices = outputs\n",
        "            else:\n",
        "                # Normal case - outputs are [batch, seq_len, vocab_size]\n",
        "                generated_indices = outputs.argmax(dim=-1)\n",
        "\n",
        "            # Slice to match target length if needed\n",
        "            if target_seq.size(1) < generated_indices.size(1):\n",
        "                generated_indices = generated_indices[:, :target_seq.size(1)]\n",
        "\n",
        "            generated_texts = decode_outputs(generated_indices, vocabulary)\n",
        "            reference_texts = decode_outputs(target_seq, vocabulary)\n",
        "\n",
        "            # Calculate ROUGE scores\n",
        "            for i in range(len(reference_texts)):\n",
        "                scores_list.append(scorer.score(reference_texts[i], generated_texts[i]))\n",
        "\n",
        "    avg_rouge = {\n",
        "        \"rouge1\": sum([s['rouge1'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "        \"rouge2\": sum([s['rouge2'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "        \"rougeL\": sum([s['rougeL'].fmeasure for s in scores_list]) / len(scores_list),\n",
        "    }\n",
        "\n",
        "    return avg_rouge\n",
        "\n",
        "\n",
        "# ====== 6. Run Training ======\n",
        "print(\"Starting Training...\")\n",
        "train(seq2seq_model, train_data_loader, optimizer, criterion)\n",
        "print(\"Training Completed!\")\n",
        "\n",
        "# ====== 7. Run Evaluation ======\n",
        "print(\"Evaluating Model...\")\n",
        "rouge_scores = evaluate(seq2seq_model, test_data_loader, vocabulary)\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:07:46.637199Z",
          "iopub.execute_input": "2025-04-09T16:07:46.637532Z",
          "iopub.status.idle": "2025-04-09T16:21:52.545886Z",
          "shell.execute_reply.started": "2025-04-09T16:07:46.637505Z",
          "shell.execute_reply": "2025-04-09T16:21:52.545062Z"
        },
        "id": "t27unlREjfFN",
        "outputId": "6b2ec6ba-df7f-491e-fc9e-11228868eefb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Vocabulary size: 9215\nStarting Training...\nEpoch 1, Training Loss: 4.7510\nEpoch 2, Training Loss: 3.5173\nEpoch 3, Training Loss: 2.8513\nTraining Completed!\nEvaluating Model...\nROUGE Scores: {'rouge1': 0.36812265512265546, 'rouge2': 0.08726984126984123, 'rougeL': 0.36812265512265546}\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C1"
      ],
      "metadata": {
        "id": "E6QYl_LojfFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# ====== 1. Load Datasets from CSV ======\n",
        "train_df = pd.read_csv(\"/kaggle/input/nlp-assign/train(1).csv\")[:-500]  # Ensure train.csv has 'text' and 'title' columns\n",
        "val_df = pd.read_csv(\"/kaggle/input/nlp-assign/train(1).csv\")[-500:]      # Ensure val.csv has 'text' and 'title' columns\n",
        "test_df = pd.read_csv(\"/kaggle/input/nlp-assign/test(1).csv\")    # Ensure test.csv has 'text' column (no titles)\n",
        "\n",
        "# ====== 2. Load Tokenizer ======\n",
        "model_name = \"google-t5/t5-small\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# ====== 3. Preprocessing Function ======\n",
        "def preprocess_function(examples):\n",
        "    model_inputs = tokenizer(examples[\"text\"], max_length=512, truncation=True, padding=\"max_length\")\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"title\"], max_length=64, truncation=True, padding=\"max_length\")\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# ====== 4. Convert DataFrames to HuggingFace Dataset ======\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Apply preprocessing\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
        "test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Wrap datasets in DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": val_dataset,\n",
        "    \"test\": test_dataset\n",
        "})\n",
        "\n",
        "# ====== 5. Load Pretrained T5 Model ======\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "\n",
        "# ====== 6. Define Training Arguments ======\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=3,\n",
        "    predict_with_generate=True\n",
        ")\n",
        "\n",
        "# ====== 7. Trainer Setup ======\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"validation\"],\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# ====== 8. Train the Model ======\n",
        "trainer.train()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "7bqNU7mpjfFO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_title_beam(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, num_beams=5, early_stopping=True)  # Beam search with 5 beams\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "\n",
        "# Generate Titles for Test Set\n",
        "test_df[\"generated_title\"] = test_df[\"text\"].apply(generate_title_beam)\n",
        "test_df.to_csv(\"test_predictions_beam.csv\", index=False)\n",
        "\n",
        "print(\"Predictions saved to test_predictions_beam.csv\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:50:20.060118Z",
          "iopub.status.idle": "2025-04-09T16:50:20.060410Z",
          "shell.execute_reply": "2025-04-09T16:50:20.060280Z"
        },
        "id": "QJ32o_H7jfFP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Load the test predictions\n",
        "# df = pd.read_csv(\"test_predictions.csv\")  # Ensure this file has 'title' & 'generated_title' columns\n",
        "df = test_df\n",
        "# Initialize ROUGE scorer\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to compute ROUGE scores\n",
        "def compute_rouge(df):\n",
        "    rouge1, rouge2, rougeL = [], [], []\n",
        "\n",
        "    for ref, gen in zip(df[\"title\"], df[\"generated_title\"]):\n",
        "        scores = scorer.score(str(ref), str(gen))  # Convert to string in case of NaN values\n",
        "        rouge1.append(scores['rouge1'].fmeasure)\n",
        "        rouge2.append(scores['rouge2'].fmeasure)\n",
        "        rougeL.append(scores['rougeL'].fmeasure)\n",
        "\n",
        "    return {\n",
        "        \"ROUGE-1\": sum(rouge1) / len(rouge1),\n",
        "        \"ROUGE-2\": sum(rouge2) / len(rouge2),\n",
        "        \"ROUGE-L\": sum(rougeL) / len(rougeL)\n",
        "    }\n",
        "\n",
        "# Compute ROUGE scores\n",
        "rouge_scores = compute_rouge(df)\n",
        "print(\"ROUGE Scores:\", rouge_scores)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-04-09T16:50:20.061187Z",
          "iopub.status.idle": "2025-04-09T16:50:20.061520Z",
          "shell.execute_reply": "2025-04-09T16:50:20.061375Z"
        },
        "id": "dwnR27uqjfFP"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}